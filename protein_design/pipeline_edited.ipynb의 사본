{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNFUMzPOEuGqBniR1JfKpyE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JlOWaMiULA2n"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/google_drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g286_klSLnlP"},"outputs":[],"source":["import importlib.util\n","import os  # Add this line to import the os module\n","\n","# Make sure utils_path is defined\n","utils_path = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion/scripts/utils/\"  # Replace with the actual path\n","\n","spec = importlib.util.spec_from_file_location(\"utils\", os.path.join(utils_path, \"utils.py\"))\n","utils = importlib.util.module_from_spec(spec)\n","spec.loader.exec_module(utils)\n","\n","print(\"Utils module imported successfully.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e578ba7a-6cc5-4cb7-9bd8-d63dbaba38af"},"outputs":[],"source":["import os, sys, glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import getpass\n","import subprocess\n","import time\n","import importlib\n","from shutil import copy2\n","\n","%load_ext autoreload\n","%autoreload 2\n","\n","### Path to this cloned GitHub repo:\n","SCRIPT_DIR = os.path.dirname(\"/content/google_drive/MyDrive/특연/heme_binder_diffusion\")  # edit this to the GitHub repo path. Throws an error by default.\n","assert os.path.exists(SCRIPT_DIR)\n","sys.path.append(SCRIPT_DIR+\"/scripts/utils\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37l9gdQ-syc_"},"outputs":[],"source":["!which python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OomsKmjmdvbw"},"outputs":[],"source":["!pip install pyrosettacolabsetup\n","import pyrosettacolabsetup; pyrosettacolabsetup.install_pyrosetta()\n","import pyrosetta; pyrosetta.init()"]},{"cell_type":"code","source":["!git clone https://github.com/RosettaCommons/RFDesign.git"],"metadata":{"id":"TKocci6ipTuN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b49d9e31"},"outputs":[],"source":["diffusion_script = \"/content/google_drive/MyDrive/특연/rf_diffusion_all_atom/run_inference.py\"  # edit this\n","inpaint_script = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion/inpaint.py\"  # edit this if needed\n","proteinMPNN_script = f\"{SCRIPT_DIR}/lib/LigandMPNN/proteinmpnn-run-script.py\"  # from submodule\n","AF2_script = f\"{SCRIPT_DIR}/scripts/af2/af2.py\"  # from submodule\n","\n","\n","\n","### Python and/or Apptainer executables needed for running the jobs\n","### Please provide paths to executables that are able to run the different tasks.\n","### They can all be the same if you have an environment with all of the ncessary Python modules in one\n","\n","# If your added Apptainer does not execute scripts directly,\n","# try adding 'apptainer run' or 'apptainer run --nv' (for GPU) in front of the command\n","\n","CONDAPATH = \"/local/bin/python\"   # edit this depending on where your Conda environments live\n","# In Colab, use the default Python path\n","PYTHON = {\n","    \"diffusion\": \"/usr/bin/python3\",\n","    \"af2\": \"/usr/bin/python3\",\n","    \"proteinMPNN\": \"/usr/bin/python3\",\n","    \"general\": \"/usr/bin/python3\"\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"59cfb5a8-5f64-49f4-a26f-0da5f09b4120"},"outputs":[],"source":["username = getpass.getuser()  # your username on the running system\n","EMAIL = f\"{username}@uw.edu\"  # edit based on your organization. For Slurm job notifications.\n","\n","PROJECT = \"example_Heme_diffusion\"\n","\n","### Path where the jobs will be run and outputs dumped\n","WDIR = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion/output\"\n","\n","if not os.path.exists(WDIR):\n","    os.makedirs(WDIR, exist_ok=True)\n","\n","print(f\"Working directory: {WDIR}\")\n","\n","USE_GPU_for_AF2 = True\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"204f8e5c-0235-478b-a123-cc896fa9734d"},"outputs":[],"source":["# Ligand information\n","params = [f\"{SCRIPT_DIR}/theozyme/HBA/HBA.params\"]  # Rosetta params file(s)\n","LIGAND = \"HBA\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7b9faacb"},"outputs":[],"source":["# Using example PDB file with ligand HBA and protein 7o2g backbone.\n","## Note: the repository also contains additional HBA conformers with 7o2g and P450 motifs\n","## in the same directory as a ZIP file.\n","\n","diffusion_inputs = glob.glob(f\"/content/google_drive/MyDrive/특연/heme_binder_diffusion/input/7o2g_HBA.pdb\")\n","print(f\"Found {len(diffusion_inputs)} PDB files\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01405838-201f-4e1e-940d-ea1d05fdaba5"},"outputs":[],"source":["## Setting up general settings for diffusion\n","\n","DIFFUSION_DIR = f\"{WDIR}/0_diffusion\"\n","if not os.path.exists(DIFFUSION_DIR):\n","    os.makedirs(DIFFUSION_DIR, exist_ok=False)\n","\n","os.chdir(DIFFUSION_DIR)\n","\n","N_designs = 5\n","T_steps = 200\n","\n","## Edit this config based on motif residues, etc...\n","config = f\"\"\"\n","defaults:\n","  - aa\n","\n","diffuser:\n","  T: {T_steps}\n","\n","inference:\n","  num_designs: {N_designs}\n","  model_runner: NRBStyleSelfCond\n","  ligand: '{LIGAND}'\n","\n","model:\n","  freeze_track_motif: True\n","\n","contigmap:\n","  contigs: [\"30-110,A15-15,30-110\"]\n","  inpaint_str: null\n","  length: \"100-140\"\n","\n","potentials:\n","  guiding_potentials: [\"type:ligand_ncontacts,weight:1\"]\n","  guide_scale: 2\n","  guide_decay: cubic\n","\"\"\"\n","\n","estimated_time = 3.5 * T_steps * N_designs  # assuming 3.5 seconds per timestep on A4000 GPU\n","\n","print(f\"Estimated time to produce {N_designs} designs = {estimated_time/60:.0f} minutes\")\n","with open(\"config.yaml\", \"w\") as file:\n","    file.write(config)\n","print(f\"Wrote config file to {os.path.realpath('config.yaml')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c8a89617-a8a7-45d5-abd5-dc85fa32bc17"},"outputs":[],"source":["## Setting up diffusion commands based on the input PDB file(s)\n","## Diffusion jobs are run in separate directories for each input PDB\n","\n","commands_diffusion = []\n","cmds_filename = \"commands_diffusion\"\n","diffusion_rundirs = []\n","with open(cmds_filename, \"w\") as file:\n","    for p in diffusion_inputs:\n","        pdbname = os.path.basename(p).replace(\".pdb\", \"\")\n","        os.makedirs(pdbname, exist_ok=True)\n","        cmd = f\"cd {pdbname} ; {PYTHON['diffusion']} {diffusion_script} --config-dir=../ \"\\\n","              f\"--config-name=config.yaml inference.input_pdb={p} \"\\\n","              f\"inference.output_prefix='./out/{pdbname}_dif' > output.log ; cd ..\\n\"\n","        commands_diffusion.append(cmd)\n","        diffusion_rundirs.append(pdbname)\n","        file.write(cmd)\n","\n","print(f\"An example diffusion command that was generated:\\n   {cmd}\")\n","\n","\n","## Creating a Slurm submit script\n","## adjust time depending on number of designs and available hardware\n","submit_script = \"submit_diffusion.sh\"\n","utils.create_slurm_submit_script(filename=submit_script, name=\"diffusion_example\", gpu=True, gres=\"gpu:a4000:1\",\n","                                 mem=\"8g\", N_cores=2, time=\"1:00:00\", email=EMAIL,\n","                                 array=len(commands_diffusion), array_commandfile=cmds_filename)\n","\n","print(f\"Writing diffusion submission script to {submit_script}\")\n","print(f\"{len(commands_diffusion)} diffusion jobs to run\")\n","\n","# Comment out the following lines to avoid the error in Google Colab\n","# if not os.path.exists(DIFFUSION_DIR+\"/.done\"):\n","#     p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","#     (output, err) = p.communicate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"86ed7eb2-c045-49a0-9264-292a6ef4dc27"},"outputs":[],"source":["## If you're done with diffusion and happy with the outputs then mark it as done\n","DIFFUSION_DIR = f\"{WDIR}/0_diffusion\"\n","os.chdir(DIFFUSION_DIR)\n","\n","if not os.path.exists(DIFFUSION_DIR+\"/.done\"):\n","    with open(f\"{DIFFUSION_DIR}/.done\", \"w\") as file:\n","        file.write(f\"Run user: {username}\\n\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"1d9Nh0EVwjgB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06c85384-baa4-41c2-bc21-42587839a44d"},"outputs":[],"source":["### Analyzing diffusion outputs for clashes, ligand burial and scaffold quality\n","## If it's running too slowly consider increasing --nproc\n","\n","analysis_script = f\"{SCRIPT_DIR}/scripts/diffusion_analysis/process_diffusion_outputs.py\"\n","\n","diffusion_outputs = []\n","for d in diffusion_rundirs:\n","    diffusion_outputs += glob.glob(f\"{d}/out/*.pdb\")\n","\n","# By default I don't use the --analyze flag. As a result the backbones are filtered as the script runs.\n","# You can set --analyze to True to calculate all scores for all backbones.\n","# This will slow the analysis down, but you can then filter the backbones separately afterwards.\n","dif_analysis_cmd_dict = {\"--pdb\": \" \".join(diffusion_outputs),\n","                        \"--ref\": f\"{SCRIPT_DIR}/input/*.pdb\",\n","                        \"--params\": \" \".join(params),\n","                        \"--term_limit\": \"15.0\",\n","                        \"--SASA_limit\": \"0.3\",  # Highest allowed relative SASA of ligand\n","                        \"--loop_limit\": \"0.4\",  # Fraction of backbone that can be loopy\n","                        \"--ref_catres\": \"A15\",  # Position of CYS in diffusion input\n","                        \"--rethread\": True,\n","                        \"--fix\": True,\n","                        \"--exclude_clash_atoms\": \"O1 O2 O3 O4 C5 C10\",  # Ligand atoms excluded from clashchecking because they are flexible\n","                        \"--ligand_exposed_atoms\": \"C45 C46 C47\",  # Ligand atoms that need to be more exposed\n","                        \"--exposed_atom_SASA\": \"10.0\",  # minimum absolute SASA for exposed ligand atoms\n","                        \"--longest_helix\": \"30\",\n","                        \"--rog\": \"30.0\",\n","                        \"--partial\": None,\n","                        \"--outdir\": None,\n","                        \"--traj\": \"5/30\",  # Also random 5 models are taken from the last 30 steps of the diffusion trajectory\n","                        \"--trb\": None,\n","                        \"--analyze\": False,\n","                        \"--nproc\": \"1\"}\n","\n","analysis_command = f\"{PYTHON['general']} {analysis_script}\"\n","for k, val in dif_analysis_cmd_dict.items():\n","    if val is not None:\n","        if isinstance(val, list):\n","            analysis_command += f\" {k}\"\n","            analysis_command += \" \" + \" \".join(val)\n","        elif isinstance(val, bool):\n","            if val == True:\n","                analysis_command += f\" {k}\"\n","        else:\n","            analysis_command += f\" {k} {val}\"\n","        print(k, val)\n","\n","if len(diffusion_outputs) < 100:\n","    ## Analyzing locally\n","    p = subprocess.Popen(analysis_command, shell=True)\n","    (output, err) = p.communicate()\n","else:\n","    ## Too many structures to analyze.\n","    ## Running the analysis as a SLURM job.\n","    submit_script = \"submit_diffusion_analysis.sh\"\n","    utils.create_slurm_submit_script(filename=submit_script, name=\"diffusion_analysis\",\n","                                     mem=\"8g\", N_cores=dif_analysis_cmd_dict[\"--nproc\"], time=\"0:20:00\", email=EMAIL,\n","                                     command=analysis_command, outfile_name=\"output_analysis\")\n","\n","diffused_backbones_good = glob.glob(f\"{DIFFUSION_DIR}/filtered_structures/*.pdb\")\n","\n","dif_analysis_df = pd.read_csv(f\"/content/google_drive/MyDrive/특연/heme_binder_diffusion/output/0_diffusion/diffusion_analysis.sc\", header=0, sep=\"\\s+\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c10b4314"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Visualizing the distributions of diffusion analysis metrics\n","# Plotting design scores\n","plt.figure(figsize=(12, 12))\n","valid_keys = [k for k in dif_analysis_df.keys() if k != \"description\"]\n","\n","# Check for finite values and filter columns\n","finite_columns = [k for k in valid_keys if np.isfinite(dif_analysis_df[k].dropna()).any()]\n","\n","for i, k in enumerate(finite_columns):\n","    plt.subplot(4, 3, i+1)\n","    plt.hist(dif_analysis_df[k].dropna())\n","    plt.title(k)\n","    plt.xlabel(k)\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JP8m8H1552cc"},"outputs":[],"source":["# Install numpy and other required packages\n","!pip install numpy pandas scipy"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FqMml56Senmv"},"outputs":[],"source":["import torch\n","\n","checkpoint = torch.load(\"/content/google_drive/MyDrive/특연/heme_binder_diffusion/lib/LigandMPNN/model_params/v_48_020.pt\")\n","for key in checkpoint.keys():\n","    print(key)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N0XvwO0cpu0D"},"outputs":[],"source":["cd \"/content/google_drive/My Drive/특연\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lsWzI_wLpxgB"},"outputs":[],"source":["!git clone https://github.com/dauparas/ProteinMPNN.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwKnUWbTp8Pg"},"outputs":[],"source":["!pip install biopython"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gZq5BjOcL_Mu"},"outputs":[],"source":["import os\n","import subprocess\n","import glob\n","\n","# 경로 설정\n","WDIR = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion\"\n","DIFFUSION_DIR = f\"{WDIR}/output/0_diffusion\"\n","MPNN_DIR = f\"/content/google_drive/MyDrive/특연/proteinmpnn/seqs\"\n","os.makedirs(MPNN_DIR, exist_ok=True)\n","\n","# ProteinMPNN 설정\n","MPNN_temperatures = [0.1, 0.2, 0.3]\n","MPNN_outputs_per_temperature = 5\n","MPNN_omit_AAs = \"C\"\n","\n","# PDB 파일 찾기\n","diffused_backbones_good = glob.glob(f\"{DIFFUSION_DIR}/filtered_structures/*.pdb\")\n","assert len(diffused_backbones_good) > 0, \"No good backbones found!\"\n","\n","def run_proteinmpnn(pdb_file, temperature):\n","    output_dir = \"/content/google_drive/MyDrive/특연/proteinmpnn/seqs\"\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    command = (f\"python /content/google_drive/MyDrive/특연/ProteinMPNN/protein_mpnn_run.py \"\n","               f\"--model_name 'v_48_030' \"  # 모델 이름을 체크포인트 파일 이름과 일치시킵니다\n","               f\"--path_to_model_weights /content/google_drive/MyDrive/특연/ProteinMPNN/vanilla_model_weights \"  # 모델 가중치 경로를 지정합니다\n","               f\"--backbone_noise 0.00 \"\n","               f\"--num_seq_per_target {MPNN_outputs_per_temperature} \"\n","               f\"--sampling_temp {temperature} \"\n","               f\"--out_folder {output_dir} \"\n","               f\"--pdb_path {pdb_file} \"\n","               f\"--chain_id A \"\n","               f\"--omit_AAs {MPNN_omit_AAs} \"\n","               f\"--seed 37 \"\n","               f\"--save_score 1 \"\n","               f\"--save_probs 1\")\n","\n","    print(f\"Executing command: {command}\")\n","\n","    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","    stdout, stderr = process.communicate()\n","\n","    print(f\"Stdout: {stdout.decode()}\")\n","    print(f\"Stderr: {stderr.decode()}\")\n","\n","    if process.returncode != 0:\n","        print(f\"Error running ProteinMPNN for {pdb_file} at temperature {temperature}\")\n","    else:\n","        print(f\"Successfully ran ProteinMPNN for {pdb_file} at temperature {temperature}\")\n","\n","# ProteinMPNN 실행\n","print(\"Starting ProteinMPNN processing...\")\n","for pdb_file in diffused_backbones_good:\n","    print(f\"Processing PDB file: {pdb_file}\")\n","    for temperature in MPNN_temperatures:\n","        print(f\"Running with temperature: {temperature}\")\n","        run_proteinmpnn(pdb_file, temperature)\n","\n","print(\"ProteinMPNN processing completed.\")\n","\n","# 결과 확인\n","output_files = glob.glob(f\"{MPNN_DIR}/seqs/*.fa\")\n","print(f\"Generated {len(output_files)} output files\")\n","print(f\"Output files: {output_files}\")\n","\n","print(\"Script execution completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1f793273"},"outputs":[],"source":["## If you're done with diffusion and happy with the outputs then mark it as done\n","MPNN_DIR = f\"{WDIR}/1_proteinmpnn\"\n","os.chdir(MPNN_DIR)\n","\n","if not os.path.exists(MPNN_DIR+\"/.done\"):\n","    with open(f\"{MPNN_DIR}/.done\", \"w\") as file:\n","        file.write(f\"Run user: {username}\\n\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZGAGTi_gnMoC"},"outputs":[],"source":["import glob\n","import os\n","\n","print(f\"MPNN_DIR: {MPNN_DIR}\")\n","print(f\"MPNN output files: {glob.glob(f'{MPNN_DIR}/seqs/*.fa')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"065029b2"},"outputs":[],"source":["os.chdir(WDIR)\n","\n","AF2_DIR = f\"{WDIR}/2_af2\"\n","os.makedirs(AF2_DIR, exist_ok=True)\n","os.chdir(AF2_DIR)\n","\n","### First collecting MPNN outputs and creating FASTA files for AF2 input\n","mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"/content/google_drive/MyDrive/특연/proteinmpnn/seqs/seqs/*.fa\"))\n","mpnn_fasta = {k: seq.strip() for k, seq in mpnn_fasta.items() if \"model_path\" not in k}  # excluding the diffused poly-A sequence\n","# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n","mpnn_fasta = {k.split(\",\")[0]+\"_\"+k.split(\",\")[2].replace(\" T=\", \"T\")+\"_0_\"+k.split(\",\")[1].replace(\" id=\", \"\"): seq for k, seq in mpnn_fasta.items()}\n","\n","print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n","\n","## Splitting the MPNN sequences based on length\n","## and grouping them in smaller batches for each AF2 job\n","## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n","\n","SEQUENCES_PER_AF2_JOB = 5  # CPU\n","if USE_GPU_for_AF2 is True:\n","    SEQUENCES_PER_AF2_JOB = 100  # GPU\n","mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"acf52a6b"},"outputs":[],"source":["import glob\n","import subprocess\n","from concurrent.futures import ThreadPoolExecutor\n","\n","# Configuration\n","AF2_recycles = 3\n","AF2_models = \"4\"  # add other models to this string if needed, i.e. \"3 4 5\"\n","cmds_filename_af2 = \"commands_af2\"\n","AF2_script = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion/scripts/af2/af2.py\"\n","PYTHON = {'af2': '/usr/bin/python3'}\n","\n","# Generate commands\n","commands_af2 = []\n","with open(cmds_filename_af2, \"w\") as file:\n","    for ff in glob.glob(\"*.fasta\"):\n","        command = (f\"{PYTHON['af2']} {AF2_script} --af-nrecycles {AF2_recycles} --af-models {AF2_models} \"\n","                   f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\\n\")\n","        commands_af2.append(command)\n","        file.write(command)\n","\n","print(\"Example AF2 command:\")\n","print(commands_af2[-1])\n","\n","# Function to run a command\n","def run_command(command):\n","    process = subprocess.Popen(command, shell=True)\n","    process.wait()\n","\n","# Run commands in parallel\n","with ThreadPoolExecutor(max_workers=4) as executor:\n","    executor.map(run_command, commands_af2)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2a88b24e"},"outputs":[],"source":["## If you're done with diffusion and happy with the outputs then mark it as done\n","AF2_DIR = f\"{WDIR}/2_af2\"\n","os.chdir(AF2_DIR)\n","\n","if not os.path.exists(AF2_DIR+\"/.done\"):\n","    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n","        file.write(f\"Run user: {username}\\n\")\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"D_wQJjCUdmow"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"9aeb0952"},"outputs":[],"source":["#이 코드블럭은 돌려보기 전에 체크\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import utils  # Ensure this module is available in your environment\n","\n","# Configuration\n","AF2_recycles = 3\n","AF2_models = \"4\"  # add other models to this string if needed, i.e. \"3 4 5\"\n","cmds_filename_af2 = \"commands_af2\"\n","AF2_script = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion/scripts/af2/af2.py\"\n","PYTHON = {\n","    \"diffusion\": sys.executable,\n","    \"af2\": sys.executable,\n","    \"proteinMPNN\": sys.executable,\n","    \"general\": sys.executable\n","}\n","WDIR = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion\"  # Define your working directory\n","SCRIPT_DIR = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion\"  # Define your script directory\n","EMAIL = \"kjju0422@snu.ac.kr\"  # Your email for notifications\n","\n","# Set up paths for analysis\n","AF2_DIR = f\"{WDIR}/2_af2\"\n","DIFFUSION_DIR = f\"{WDIR}/0_diffusion\"\n","\n","# Change to the AF2_DIR\n","os.chdir(AF2_DIR)\n","\n","# Load the provided CSV file\n","file_path = \"/content/google_drive/MyDrive/특연/heme_binder_diffusion/2_af2/scores.csv\"\n","scores_af2 = pd.read_csv(file_path, sep=\",\")\n","\n","# Remove the first descriptive row if it exists\n","if scores_af2.iloc[0, 0] == \"description\":\n","    scores_af2 = scores_af2.drop(0).reset_index(drop=True)\n","\n","# Convert columns to numeric where applicable\n","scores_af2[['lDDT', 'Time', 'rmsd', 'rmsd_SR1']] = scores_af2[['lDDT', 'Time', 'rmsd', 'rmsd_SR1']].apply(pd.to_numeric, errors='coerce')\n","\n","# Inspect the columns and a few rows\n","print(\"Columns in the dataframe:\")\n","print(scores_af2.columns)\n","\n","print(\"\\nFirst few rows of the dataframe:\")\n","print(scores_af2.head())\n","\n","print(f\"\\nShape of the dataframe: {scores_af2.shape}\")\n","\n","# Adjust filter criteria\n","AF2_filters = {}\n","if \"lDDT\" in scores_af2.columns:\n","    AF2_filters[\"lDDT\"] = [2.0, \">=\"]  # Relaxed criteria\n","if \"rmsd\" in scores_af2.columns:\n","    AF2_filters[\"rmsd\"] = [30.0, \"<=\"]  # Relaxed criteria\n","if \"rmsd_SR1\" in scores_af2.columns:\n","    AF2_filters[\"rmsd_SR1\"] = [5.0, \"<=\"]\n","\n","# Filter scores and plot only if data is available\n","if AF2_filters:\n","    scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n","    if not scores_af2_filtered.empty:\n","        utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n","\n","        # Clean data before plotting\n","        scores_af2_filtered = scores_af2_filtered.replace([np.inf, -np.inf], np.nan).dropna(subset=AF2_filters.keys())\n","\n","        # Plotting AF2 scores\n","        plt.figure(figsize=(12, 3))\n","        for i, k in enumerate(AF2_filters):\n","            plt.subplot(1, len(AF2_filters), i+1)\n","            plt.hist(scores_af2_filtered[k])\n","            plt.title(k)\n","            plt.xlabel(k)\n","        plt.tight_layout()\n","        plt.savefig(\"af2_scores_histogram.png\")\n","        plt.close()\n","\n","        if \"lDDT\" in AF2_filters and \"rmsd\" in AF2_filters:\n","            utils.plot_score_pairs(scores_af2_filtered, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])\n","            plt.savefig(\"af2_scores_scatter.png\")\n","            plt.close()\n","    else:\n","        print(\"No data available after applying filters.\")\n","else:\n","    print(\"Warning: No matching columns found for filtering.\")\n","\n","# Prepare analysis command\n","params = [\"param1\", \"param2\"]  # Define your parameters if needed\n","analysis_cmd = (f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile {file_path} \"\n","                f\"--ref_path {DIFFUSION_DIR}/filtered_structures/ --mpnn --params {' '.join(params)}\")\n","\n","print(\"Analysis command:\")\n","print(analysis_cmd)\n","\n","# Function to run analysis\n","def run_analysis(command):\n","    try:\n","        result = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n","        print(\"Analysis output:\")\n","        print(result.stdout)\n","        print(\"Analysis errors:\")\n","        print(result.stderr)\n","    except subprocess.CalledProcessError as e:\n","        print(\"Error during analysis command execution:\")\n","        print(e.stdout)\n","        print(e.stderr)\n","\n","# Run analysis based on the number of PDB files\n","if len(glob.glob(f\"{AF2_DIR}/*.pdb\")) < 100:\n","    # Analyzing locally\n","    run_analysis(analysis_cmd)\n","else:\n","    # Running as a Slurm job\n","    submit_script = \"submit_af2_analysis.sh\"\n","    utils.create_slurm_submit_script(filename=submit_script, name=\"af2_analysis\",\n","                                     mem=\"16g\", N_cores=8, time=\"0:20:00\", email=EMAIL,\n","                                     command=analysis_cmd, outfile_name=\"output_analysis\")\n","\n","    p = subprocess.Popen([\"sbatch\", submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","    output, err = p.communicate()\n","    print(\"SLURM job submission output:\", output.decode())\n","    print(\"SLURM job submission errors:\", err.decode())\n","\n","print(f\"\\nContent of {file_path}:\")\n","print(scores_af2.head())\n","print(f\"\\nShape of the dataframe: {scores_af2.shape}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b8e730b3"},"outputs":[],"source":["import os\n","import glob\n","import subprocess\n","\n","WDIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"  # Define your working directory\n","SCRIPT_DIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"  # Define your script directory\n","EMAIL = \"kjju0422@snu.ac.kr\"  # Your email for notifications\n","\n","os.chdir(WDIR)\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","os.makedirs(DESIGN_DIR_ligMPNN, exist_ok=True)\n","os.chdir(DESIGN_DIR_ligMPNN)\n","\n","AF2_DIR = f\"{WDIR}/2_af2\"\n","os.makedirs(f\"{DESIGN_DIR_ligMPNN}/logs\", exist_ok=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vvydo4Smxpm6"},"outputs":[],"source":["NSTRUCT = 10\n","cstfile = f\"{SCRIPT_DIR}/theozyme/HBA/HBA_CYS_UPO.cst\"\n","params = [\"param1\", \"param2\"]  # Add your actual parameters here\n","\n","commands_design = []\n","cmds_filename_des = \"commands_design\"\n","with open(cmds_filename_des, \"w\") as file:\n","    for pdb in glob.glob(f\"{AF2_DIR}/good/*.pdb\"):\n","        command = (\n","            f\"/content/drive/MyDrive/특연/heme_binder_diffusion/scripts/design/heme_pocket_ligMPNN.py\"\n","            f\"--pdb {pdb} --nstruct {NSTRUCT} \"\n","            f\"--scoring {SCRIPT_DIR}/scripts/design/scoring/heme_scoring.py \"\n","            f\"--params {' '.join(params)} --cstfile {cstfile} > logs/{os.path.basename(pdb).replace('.pdb', '.log')}\\n\"\n","        )\n","        commands_design.append(command)\n","        file.write(command)\n","\n","print(\"Example design command:\")\n","print(commands_design[-1])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CXsfdi_Wxtky"},"outputs":[],"source":["submit_script = \"submit_design.sh\"\n","utils.create_slurm_submit_script(\n","    filename=submit_script,\n","    name=\"3.1_design_pocket_ligMPNN\",\n","    mem=\"4g\",\n","    N_cores=1,\n","    time=\"3:00:00\",\n","    email=EMAIL,\n","    array=len(commands_design),\n","    array_commandfile=cmds_filename_des\n",")\n","\n","if not os.path.exists(f\"{DESIGN_DIR_ligMPNN}/.done\"):\n","    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","    output, err = p.communicate()\n","    print(\"SLURM job submission output:\", output.decode())\n","    print(\"SLURM job submission errors:\", err.decode())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43c2821f"},"outputs":[],"source":["## If you're done with design and happy with the outputs then mark it as done\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","os.chdir(DESIGN_DIR_ligMPNN)\n","\n","if not os.path.exists(DESIGN_DIR_ligMPNN+\"/.done\"):\n","    with open(f\"{DESIGN_DIR_ligMPNN}/.done\", \"w\") as file:\n","        file.write(f\"Run user: {username}\\n\")"]},{"cell_type":"code","source":[],"metadata":{"id":"HtyEMKQLw1HJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e2364e89"},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Define the path to your data file\n","data_file = \"/content/drive/MyDrive/특연/heme_binder_diffusion/2_af2/filtered_scores.sc\"\n","\n","# Check if the file exists\n","if not os.path.exists(data_file):\n","    sys.exit(\"Design job failed, or no successful outputs were produced.\")\n","\n","# Read the data\n","scores = pd.read_csv(data_file, sep=\"\\s+\", header=0)\n","\n","# Print column names and data types\n","print(\"Columns and their data types:\")\n","print(scores.dtypes)\n","\n","# Print the first few rows of the dataset\n","print(\"\\nFirst few rows of the dataset:\")\n","print(scores.head())\n","\n","# Check for missing values\n","print(\"\\nMissing values in each column:\")\n","print(scores.isnull().sum())\n","\n","# Function to plot histogram for numeric columns\n","def plot_numeric_histogram(df, column):\n","    data = df[column].dropna()\n","    if len(data) > 0:\n","        plt.hist(data)\n","        plt.title(column)\n","        plt.xlabel(column)\n","    else:\n","        plt.text(0.5, 0.5, f\"No numeric data in {column}\", ha='center', va='center')\n","\n","# Plotting histograms for numeric columns\n","numeric_columns = scores.select_dtypes(include=[np.number]).columns\n","num_cols = len(numeric_columns)\n","rows = (num_cols + 1) // 2\n","plt.figure(figsize=(15, 5 * rows))\n","\n","for i, column in enumerate(numeric_columns, 1):\n","    plt.subplot(rows, 2, i)\n","    plot_numeric_histogram(scores, column)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print summary statistics for numeric columns\n","print(\"\\nSummary Statistics:\")\n","print(scores[numeric_columns].describe())\n","\n","# Print unique values for non-numeric columns\n","for column in scores.select_dtypes(exclude=[np.number]).columns:\n","    print(f\"\\nUnique values in {column}:\")\n","    print(scores[column].unique())\n","\n","# Correlation matrix for numeric columns\n","correlation_matrix = scores[numeric_columns].corr()\n","print(\"\\nCorrelation Matrix:\")\n","print(correlation_matrix)\n","\n","# Heatmap of correlation matrix\n","plt.figure(figsize=(10, 8))\n","plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n","plt.colorbar()\n","plt.xticks(range(len(numeric_columns)), numeric_columns, rotation=90)\n","plt.yticks(range(len(numeric_columns)), numeric_columns)\n","plt.title(\"Correlation Heatmap\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0wY3RTicSWIQ"},"outputs":[],"source":["import os\n","import sys\n","import requests\n","from tqdm import tqdm\n","\n","def download_file(url, filename):\n","    \"\"\"\n","    Download a file from the given URL and save it to the specified filename.\n","    Show a progress bar during download.\n","    \"\"\"\n","    response = requests.get(url, stream=True)\n","    total_size = int(response.headers.get('content-length', 0))\n","\n","    with open(filename, 'wb') as file, tqdm(\n","        desc=filename,\n","        total=total_size,\n","        unit='iB',\n","        unit_scale=True,\n","        unit_divisor=1024,\n","    ) as progress_bar:\n","        for data in response.iter_content(chunk_size=1024):\n","            size = file.write(data)\n","            progress_bar.update(size)\n","\n","# Define the path to store model files\n","WDIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"\n","MODEL_DIR = f\"{WDIR}/lib/LigandMPNN/model_params\"\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","# List of LigandMPNN model files to download\n","model_files = [\n","    \"ligandmpnn_v_32_005_25.pt\",\n","    \"ligandmpnn_v_32_010_25.pt\",\n","    \"ligandmpnn_v_32_020_25.pt\",\n","    \"ligandmpnn_v_32_030_25.pt\"\n","]\n","\n","# Download each model file\n","for model_file in model_files:\n","    model_url = f\"https://files.ipd.uw.edu/pub/ligandmpnn/{model_file}\"\n","    model_path = os.path.join(MODEL_DIR, model_file)\n","\n","    if not os.path.exists(model_path):\n","        print(f\"Downloading {model_file}...\")\n","        try:\n","            download_file(model_url, model_path)\n","            print(f\"Successfully downloaded {model_file}\")\n","        except Exception as e:\n","            print(f\"Failed to download {model_file}: {str(e)}\")\n","    else:\n","        print(f\"{model_file} already exists, skipping download.\")\n","\n","print(\"\\nDownload process completed.\")\n","print(f\"Model files are located in: {MODEL_DIR}\")\n","\n","# Verify the downloaded files\n","print(\"\\nVerifying downloaded files:\")\n","for model_file in model_files:\n","    model_path = os.path.join(MODEL_DIR, model_file)\n","    if os.path.exists(model_path):\n","        print(f\"  {model_file}: Found\")\n","    else:\n","        print(f\"  {model_file}: Not found\")\n","\n","print(\"\\nYou can now proceed with running the LigandMPNN script.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6YBC0HDAX_e-"},"outputs":[],"source":["!python /content/google_drive/MyDrive/특연/heme_binder_diffusion/lib/LigandMPNN/proteinmpnn-run-script.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1f470af9"},"outputs":[],"source":["import os\n","import sys\n","import glob\n","import subprocess\n","\n","# Define necessary paths and parameters\n","WDIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n","SCRIPT_DIR = f\"{WDIR}/scripts\"\n","LIGAND = \"HBA\"\n","PYTHON = {\"general\": \"python\", \"proteinMPNN\": \"python\"}\n","\n","# Create necessary directories\n","os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n","os.chdir(DESIGN_DIR_2nd_mpnn)\n","\n","# Check if there are designs to run MPNN on\n","pdb_files = glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\")\n","if len(pdb_files) == 0:\n","    sys.exit(\"No designs to run 2nd MPNN on.\")\n","\n","# Print the PDB files found\n","print(\"PDB files found:\")\n","for pdb_file in pdb_files:\n","    print(pdb_file)\n","\n","# Check if setup script exists\n","setup_script_path = f\"{SCRIPT_DIR}/design/setup_ligand_mpnn_2nd_layer.py\"\n","if not os.path.exists(setup_script_path):\n","    print(f\"Setup script not found at: {setup_script_path}\")\n","    print(\"Current working directory:\", os.getcwd())\n","    print(\"Contents of SCRIPT_DIR:\")\n","    print(os.listdir(SCRIPT_DIR))\n","    sys.exit(\"Setup script not found\")\n","\n","# Setup for LigandMPNN\n","make_json_cmd = f\"{PYTHON['general']} {setup_script_path} \" \\\n","                f\"--ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \" \\\n","                f\"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \" \\\n","                f\"--pdb {' '.join(pdb_files)}\"\n","\n","print(\"Running setup command:\")\n","print(make_json_cmd)\n","\n","try:\n","    result = subprocess.run(make_json_cmd, shell=True, check=True, capture_output=True, text=True)\n","    print(\"Setup command output:\")\n","    print(result.stdout)\n","except subprocess.CalledProcessError as e:\n","    print(\"Error in setup command execution:\")\n","    print(f\"Return code: {e.returncode}\")\n","    print(\"Standard output:\")\n","    print(e.stdout)\n","    print(\"Standard error:\")\n","    print(e.stderr)\n","    sys.exit(\"Setup command failed\")\n","\n","print(\"Setup completed successfully\")\n","\n","# The rest of your LigandMPNN execution code would go here...\n","\n","print(\"LigandMPNN process finished.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"njpfx4vzZs_z"},"outputs":[],"source":["import os\n","import sys\n","import glob\n","import subprocess\n","\n","# Define necessary paths and parameters\n","WDIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n","SCRIPT_DIR = f\"{WDIR}/scripts\"\n","LIGAND = \"HBA\"\n","PYTHON = {\"general\": \"python\", \"proteinMPNN\": \"python\"}\n","proteinMPNN_script = f\"{WDIR}/lib/LigandMPNN/proteinmpnn-run-script.py\"\n","\n","# Create necessary directories\n","os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n","os.chdir(DESIGN_DIR_2nd_mpnn)\n","\n","# Check if there are designs to run MPNN on\n","pdb_files = glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\")\n","if len(pdb_files) == 0:\n","    sys.exit(\"No designs to run 2nd MPNN on.\")\n","\n","# Print the PDB files found\n","print(\"PDB files found:\")\n","for pdb_file in pdb_files:\n","    print(pdb_file)\n","\n","# Check if setup script exists\n","setup_script_path = f\"{SCRIPT_DIR}/design/setup_ligand_mpnn_2nd_layer.py\"\n","if not os.path.exists(setup_script_path):\n","    print(f\"Setup script not found at: {setup_script_path}\")\n","    print(\"Current working directory:\", os.getcwd())\n","    print(\"Contents of SCRIPT_DIR:\")\n","    print(os.listdir(SCRIPT_DIR))\n","    sys.exit(\"Setup script not found\")\n","\n","# Setup for LigandMPNN\n","make_json_cmd = f\"{PYTHON['general']} {setup_script_path} \" \\\n","                f\"--ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \" \\\n","                f\"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \" \\\n","                f\"--pdb {' '.join(pdb_files)}\"\n","\n","print(\"Running setup command:\")\n","print(make_json_cmd)\n","\n","try:\n","    result = subprocess.run(make_json_cmd, shell=True, check=True, capture_output=True, text=True)\n","    print(\"Setup command output:\")\n","    print(result.stdout)\n","except subprocess.CalledProcessError as e:\n","    print(\"Error in setup command execution:\")\n","    print(f\"Return code: {e.returncode}\")\n","    print(\"Standard output:\")\n","    print(e.stdout)\n","    print(\"Standard error:\")\n","    print(e.stderr)\n","    sys.exit(\"Setup command failed\")\n","\n","print(\"Setup completed successfully\")\n","\n","# LigandMPNN parameters\n","MPNN_temperatures = [0.1, 0.2]\n","MPNN_outputs_per_temperature = 5\n","MPNN_omit_AAs = \"CM\"\n","\n","# Function to run LigandMPNN\n","def run_ligandmpnn(pdb_file, temperature):\n","    cmd = f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \" \\\n","          f\"--model_type ligand_mpnn --ligand_mpnn_use_atom_context 1 \" \\\n","          f\"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \" \\\n","          f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {temperature} \" \\\n","          f\"--omit_AA {MPNN_omit_AAs} --pdb_path {pdb_file} \" \\\n","          f\"--checkpoint_protein_mpnn {WDIR}/lib/LigandMPNN/model_params/ligandmpnn_v_32_010_25.pt\"\n","\n","    print(f\"Running LigandMPNN for {os.path.basename(pdb_file)} at temperature {temperature}\")\n","    print(cmd)\n","    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n","\n","    if result.returncode != 0:\n","        print(\"Error in LigandMPNN execution:\")\n","        print(result.stderr)\n","    else:\n","        print(\"LigandMPNN execution completed\")\n","\n","# Run LigandMPNN for each PDB file and temperature\n","for pdb_file in pdb_files:\n","    for temp in MPNN_temperatures:\n","        run_ligandmpnn(pdb_file, temp)\n","\n","# Create a .done file to indicate completion, regardless of success\n","with open(f\"{DESIGN_DIR_2nd_mpnn}/.done\", \"w\") as f:\n","    f.write(\"LigandMPNN runs completed\")\n","\n","print(\"LigandMPNN process finished. .done file created.\")\n","\n","# Verify the process\n","if os.path.exists(f\"{DESIGN_DIR_2nd_mpnn}/.done\"):\n","    print(\"2nd MPNN has been marked as completed.\")\n","else:\n","    print(\"2nd MPNN completion marker not found. This should not happen.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"15e96fe9"},"outputs":[],"source":["os.chdir(WDIR)\n","DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n","assert os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"), \"2nd MPNN has not been performed!\"\n","\n","AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n","os.makedirs(AF2_DIR, exist_ok=True)\n","os.chdir(AF2_DIR)\n","\n","### First collecting MPNN outputs and creating FASTA files for AF2 input\n","mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{DESIGN_DIR_2nd_mpnn}/seqs/*.fasta\"))\n","# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n","_mpnn_fasta = {}\n","for k, seq in mpnn_fasta.items():\n","    parts = k.split(\"_\")\n","    if \"global_score\" in k:\n","        _mpnn_fasta[f\"{parts[0]}_{parts[1]}_native\"] = seq.strip()\n","    else:\n","        temperature = parts[0].replace(\"T=\", \"T\")\n","        score = parts[1].split(\"=\")[1]\n","        sample = parts[3].replace(\"sample=\", \"\")\n","        _mpnn_fasta[f\"{temperature}_{score}_{sample}\"] = seq.strip()\n","\n","mpnn_fasta = {k:v for k,v in _mpnn_fasta.items()}\n","\n","print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n","\n","## Splitting the MPNN sequences based on length\n","## and grouping them in smaller batches for each AF2 job\n","## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n","SEQUENCES_PER_AF2_JOB = 5  # CPU\n","if USE_GPU_for_AF2 is True:\n","    SEQUENCES_PER_AF2_JOB = 100  # GPU\n","mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWiXeRiMlmlK"},"outputs":[],"source":["print(\"PYTHON dictionary contents:\")\n","print(PYTHON)\n","PYTHON['af2'] = \"python\"  # 또는 AlphaFold2용 Python 인터프리터의 전체 경로\n","PYTHON['af2'] = \"conda run -n alphafold python\"  # Conda 환경 'alphafold'를 사용하는 경우\n","print(\"AF2_script:\", AF2_script)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6d082ac"},"outputs":[],"source":["import os\n","import glob\n","\n","# AlphaFold2 설정\n","AF2_recycles = 3\n","AF2_models = \"4\"  # 필요한 경우 다른 모델을 이 문자열에 추가하세요\n","\n","# AlphaFold2 스크립트 경로 설정 (실제 경로로 변경해야 합니다)\n","AF2_script = \"/content/drive/MyDrive/특연/heme_binder_diffusion/scripts/af2/af2.py\"\n","\n","# Python 실행 경로 설정\n","PYTHON = {\"af2\": \"python\"}  # 또는 AlphaFold2를 실행할 Python 환경의 경로\n","\n","# AlphaFold2 명령어 생성\n","commands_af2 = []\n","for ff in glob.glob(\"*.fasta\"):\n","    cmd = f\"{PYTHON['af2']} {AF2_script} \" \\\n","          f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \" \\\n","          f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\"\n","    commands_af2.append(cmd)\n","\n","print(f\"총 {len(commands_af2)}개의 AlphaFold2 작업이 준비되었습니다.\")\n","print(\"예시 AlphaFold2 명령어:\")\n","print(commands_af2[0] if commands_af2 else \"명령어가 생성되지 않았습니다.\")\n","\n","# AlphaFold2 실행 (주의: 시간이 오래 걸릴 수 있습니다)\n","for i, cmd in enumerate(commands_af2, 1):\n","    print(f\"작업 {i}/{len(commands_af2)} 실행 중...\")\n","    os.system(cmd)\n","\n","print(\"모든 AlphaFold2 작업이 완료되었습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"98095a31"},"outputs":[],"source":["## If you're done with AF2 and happy with the outputs then mark it as done\n","AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n","os.chdir(AF2_DIR)\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","\n","if not os.path.exists(AF2_DIR+\"/.done\"):\n","    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n","        file.write(f\"Run user: {username}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2fd05f9"},"outputs":[],"source":["PYTHON = {\n","    \"diffusion\": \"/usr/bin/python3\",\n","    \"af2\": \"/usr/bin/python3\",\n","    \"proteinMPNN\": \"/usr/bin/python3\",\n","    \"general\": \"/usr/bin/python3\"\n","}\n","\n","# Combining all CSV scorefiles into one\n","os.system(\"head -n 1 $(ls *aa*.csv | shuf -n 1) > scores.csv ; for f in *aa*.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n","assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n","\n","### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n","### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n","\n","analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n","               f\"--ref_path {DESIGN_DIR_ligMPNN}/good/ --mpnn --params {' '.join(params)}\"\n","\n","p = subprocess.Popen(analysis_cmd, shell=True)\n","(output, err) = p.communicate()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA__iIj54zA5"},"outputs":[],"source":["scores_af2 = pd.read_csv(\"scores.sc\", sep=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XaK8aJn45FMU"},"outputs":[],"source":["print(\"Columns in scores_af2:\")\n","print(scores_af2.columns)\n","print(\"\\nFirst few rows of scores_af2:\")\n","print(scores_af2.head())\n","\n","# 필요한 경우 열 이름 공백 제거\n","scores_af2.columns = scores_af2.columns.str.strip()\n","\n","# 데이터 타입 변환 (필요한 경우)\n","scores_af2['lDDT'] = pd.to_numeric(scores_af2['lDDT'], errors='coerce')\n","scores_af2['rmsd'] = pd.to_numeric(scores_af2['rmsd'], errors='coerce')\n","scores_af2['rmsd_SR1'] = pd.to_numeric(scores_af2['rmsd_SR1'], errors='coerce')\n","\n","print(\"\\nUpdated columns in scores_af2:\")\n","print(scores_af2.columns)\n","print(\"\\nUpdated first few rows of scores_af2:\")\n","print(scores_af2.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1YxIUt47TgE"},"outputs":[],"source":["# NaN 값 제거\n","scores_af2 = scores_af2.dropna(subset=['lDDT', 'rmsd'])\n","\n","# 데이터 타입 변환\n","scores_af2['lDDT'] = pd.to_numeric(scores_af2['lDDT'], errors='coerce')\n","scores_af2['rmsd'] = pd.to_numeric(scores_af2['rmsd'], errors='coerce')\n","\n","# 이상치 제거 (선택사항)\n","q_low = scores_af2['lDDT'].quantile(0.01)\n","q_high = scores_af2['lDDT'].quantile(0.99)\n","scores_af2 = scores_af2[(scores_af2['lDDT'] > q_low) & (scores_af2['lDDT'] < q_high)]\n","\n","q_low = scores_af2['rmsd'].quantile(0.01)\n","q_high = scores_af2['rmsd'].quantile(0.99)\n","scores_af2 = scores_af2[(scores_af2['rmsd'] > q_low) & (scores_af2['rmsd'] < q_high)]"]},{"cell_type":"code","source":[],"metadata":{"id":"pVX0nj1YxAgj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ItqkNEPxDSj"},"outputs":[],"source":["import os\n","import sys\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Define the path to your data file\n","data_file = \"/content/drive/MyDrive/특연/heme_binder_diffusion/2_af2/filtered_scores.sc\"\n","\n","# Check if the file exists\n","if not os.path.exists(data_file):\n","    sys.exit(\"Design job failed, or no successful outputs were produced.\")\n","\n","# Read the data\n","scores = pd.read_csv(data_file, sep=\"\\s+\", header=0)\n","\n","# Print column names and data types\n","print(\"Columns and their data types:\")\n","print(scores.dtypes)\n","\n","# Print the first few rows of the dataset\n","print(\"\\nFirst few rows of the dataset:\")\n","print(scores.head())\n","\n","# Check for missing values\n","print(\"\\nMissing values in each column:\")\n","print(scores.isnull().sum())\n","\n","# Function to plot histogram for numeric columns\n","def plot_numeric_histogram(df, column):\n","    data = df[column].dropna()\n","    if len(data) > 0:\n","        plt.hist(data)\n","        plt.title(column)\n","        plt.xlabel(column)\n","    else:\n","        plt.text(0.5, 0.5, f\"No numeric data in {column}\", ha='center', va='center')\n","\n","# Plotting histograms for numeric columns\n","numeric_columns = scores.select_dtypes(include=[np.number]).columns\n","num_cols = len(numeric_columns)\n","rows = (num_cols + 1) // 2\n","plt.figure(figsize=(15, 5 * rows))\n","\n","for i, column in enumerate(numeric_columns, 1):\n","    plt.subplot(rows, 2, i)\n","    plot_numeric_histogram(scores, column)\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Print summary statistics for numeric columns\n","print(\"\\nSummary Statistics:\")\n","print(scores[numeric_columns].describe())\n","\n","# Print unique values for non-numeric columns\n","for column in scores.select_dtypes(exclude=[np.number]).columns:\n","    print(f\"\\nUnique values in {column}:\")\n","    print(scores[column].unique())\n","\n","# Correlation matrix for numeric columns\n","correlation_matrix = scores[numeric_columns].corr()\n","print(\"\\nCorrelation Matrix:\")\n","print(correlation_matrix)\n","\n","# Heatmap of correlation matrix\n","plt.figure(figsize=(10, 8))\n","plt.imshow(correlation_matrix, cmap='coolwarm', aspect='auto')\n","plt.colorbar()\n","plt.xticks(range(len(numeric_columns)), numeric_columns, rotation=90)\n","plt.yticks(range(len(numeric_columns)), numeric_columns)\n","plt.title(\"Correlation Heatmap\")\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MYaJrPksxDSk"},"outputs":[],"source":["import os\n","import sys\n","import requests\n","from tqdm import tqdm\n","\n","def download_file(url, filename):\n","    \"\"\"\n","    Download a file from the given URL and save it to the specified filename.\n","    Show a progress bar during download.\n","    \"\"\"\n","    response = requests.get(url, stream=True)\n","    total_size = int(response.headers.get('content-length', 0))\n","\n","    with open(filename, 'wb') as file, tqdm(\n","        desc=filename,\n","        total=total_size,\n","        unit='iB',\n","        unit_scale=True,\n","        unit_divisor=1024,\n","    ) as progress_bar:\n","        for data in response.iter_content(chunk_size=1024):\n","            size = file.write(data)\n","            progress_bar.update(size)\n","\n","# Define the path to store model files\n","WDIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"\n","MODEL_DIR = f\"{WDIR}/lib/LigandMPNN/model_params\"\n","\n","# Create the directory if it doesn't exist\n","os.makedirs(MODEL_DIR, exist_ok=True)\n","\n","# List of LigandMPNN model files to download\n","model_files = [\n","    \"ligandmpnn_v_32_005_25.pt\",\n","    \"ligandmpnn_v_32_010_25.pt\",\n","    \"ligandmpnn_v_32_020_25.pt\",\n","    \"ligandmpnn_v_32_030_25.pt\"\n","]\n","\n","# Download each model file\n","for model_file in model_files:\n","    model_url = f\"https://files.ipd.uw.edu/pub/ligandmpnn/{model_file}\"\n","    model_path = os.path.join(MODEL_DIR, model_file)\n","\n","    if not os.path.exists(model_path):\n","        print(f\"Downloading {model_file}...\")\n","        try:\n","            download_file(model_url, model_path)\n","            print(f\"Successfully downloaded {model_file}\")\n","        except Exception as e:\n","            print(f\"Failed to download {model_file}: {str(e)}\")\n","    else:\n","        print(f\"{model_file} already exists, skipping download.\")\n","\n","print(\"\\nDownload process completed.\")\n","print(f\"Model files are located in: {MODEL_DIR}\")\n","\n","# Verify the downloaded files\n","print(\"\\nVerifying downloaded files:\")\n","for model_file in model_files:\n","    model_path = os.path.join(MODEL_DIR, model_file)\n","    if os.path.exists(model_path):\n","        print(f\"  {model_file}: Found\")\n","    else:\n","        print(f\"  {model_file}: Not found\")\n","\n","print(\"\\nYou can now proceed with running the LigandMPNN script.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ibao0RtPxDSl"},"outputs":[],"source":["!python /content/google_drive/MyDrive/특연/heme_binder_diffusion/lib/LigandMPNN/proteinmpnn-run-script.py"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fqnFzyYOxDSl"},"outputs":[],"source":["import os\n","import sys\n","import glob\n","import subprocess\n","\n","# Define necessary paths and parameters\n","WDIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n","SCRIPT_DIR = f\"{WDIR}/scripts\"\n","LIGAND = \"HBA\"\n","PYTHON = {\"general\": \"python\", \"proteinMPNN\": \"python\"}\n","\n","# Create necessary directories\n","os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n","os.chdir(DESIGN_DIR_2nd_mpnn)\n","\n","# Check if there are designs to run MPNN on\n","pdb_files = glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\")\n","if len(pdb_files) == 0:\n","    sys.exit(\"No designs to run 2nd MPNN on.\")\n","\n","# Print the PDB files found\n","print(\"PDB files found:\")\n","for pdb_file in pdb_files:\n","    print(pdb_file)\n","\n","# Check if setup script exists\n","setup_script_path = f\"{SCRIPT_DIR}/design/setup_ligand_mpnn_2nd_layer.py\"\n","if not os.path.exists(setup_script_path):\n","    print(f\"Setup script not found at: {setup_script_path}\")\n","    print(\"Current working directory:\", os.getcwd())\n","    print(\"Contents of SCRIPT_DIR:\")\n","    print(os.listdir(SCRIPT_DIR))\n","    sys.exit(\"Setup script not found\")\n","\n","# Setup for LigandMPNN\n","make_json_cmd = f\"{PYTHON['general']} {setup_script_path} \" \\\n","                f\"--ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \" \\\n","                f\"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \" \\\n","                f\"--pdb {' '.join(pdb_files)}\"\n","\n","print(\"Running setup command:\")\n","print(make_json_cmd)\n","\n","try:\n","    result = subprocess.run(make_json_cmd, shell=True, check=True, capture_output=True, text=True)\n","    print(\"Setup command output:\")\n","    print(result.stdout)\n","except subprocess.CalledProcessError as e:\n","    print(\"Error in setup command execution:\")\n","    print(f\"Return code: {e.returncode}\")\n","    print(\"Standard output:\")\n","    print(e.stdout)\n","    print(\"Standard error:\")\n","    print(e.stderr)\n","    sys.exit(\"Setup command failed\")\n","\n","print(\"Setup completed successfully\")\n","\n","# The rest of your LigandMPNN execution code would go here...\n","\n","print(\"LigandMPNN process finished.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mmlbXQcnxDSm"},"outputs":[],"source":["import os\n","import sys\n","import glob\n","import subprocess\n","\n","# Define necessary paths and parameters\n","WDIR = \"/content/drive/MyDrive/특연/heme_binder_diffusion\"\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n","SCRIPT_DIR = f\"{WDIR}/scripts\"\n","LIGAND = \"HBA\"\n","PYTHON = {\"general\": \"python\", \"proteinMPNN\": \"python\"}\n","proteinMPNN_script = f\"{WDIR}/lib/LigandMPNN/proteinmpnn-run-script.py\"\n","\n","# Create necessary directories\n","os.makedirs(DESIGN_DIR_2nd_mpnn, exist_ok=True)\n","os.chdir(DESIGN_DIR_2nd_mpnn)\n","\n","# Check if there are designs to run MPNN on\n","pdb_files = glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\")\n","if len(pdb_files) == 0:\n","    sys.exit(\"No designs to run 2nd MPNN on.\")\n","\n","# Print the PDB files found\n","print(\"PDB files found:\")\n","for pdb_file in pdb_files:\n","    print(pdb_file)\n","\n","# Check if setup script exists\n","setup_script_path = f\"{SCRIPT_DIR}/design/setup_ligand_mpnn_2nd_layer.py\"\n","if not os.path.exists(setup_script_path):\n","    print(f\"Setup script not found at: {setup_script_path}\")\n","    print(\"Current working directory:\", os.getcwd())\n","    print(\"Contents of SCRIPT_DIR:\")\n","    print(os.listdir(SCRIPT_DIR))\n","    sys.exit(\"Setup script not found\")\n","\n","# Setup for LigandMPNN\n","make_json_cmd = f\"{PYTHON['general']} {setup_script_path} \" \\\n","                f\"--ligand {LIGAND} --output_path parsed_pdbs_lig.jsonl \" \\\n","                f\"--output_path masked_pos.jsonl --dist_bb 6.0 --dist_sc 5.0 \" \\\n","                f\"--pdb {' '.join(pdb_files)}\"\n","\n","print(\"Running setup command:\")\n","print(make_json_cmd)\n","\n","try:\n","    result = subprocess.run(make_json_cmd, shell=True, check=True, capture_output=True, text=True)\n","    print(\"Setup command output:\")\n","    print(result.stdout)\n","except subprocess.CalledProcessError as e:\n","    print(\"Error in setup command execution:\")\n","    print(f\"Return code: {e.returncode}\")\n","    print(\"Standard output:\")\n","    print(e.stdout)\n","    print(\"Standard error:\")\n","    print(e.stderr)\n","    sys.exit(\"Setup command failed\")\n","\n","print(\"Setup completed successfully\")\n","\n","# LigandMPNN parameters\n","MPNN_temperatures = [0.1, 0.2]\n","MPNN_outputs_per_temperature = 5\n","MPNN_omit_AAs = \"CM\"\n","\n","# Function to run LigandMPNN\n","def run_ligandmpnn(pdb_file, temperature):\n","    cmd = f\"{PYTHON['proteinMPNN']} {proteinMPNN_script} \" \\\n","          f\"--model_type ligand_mpnn --ligand_mpnn_use_atom_context 1 \" \\\n","          f\"--fixed_residues_multi masked_pos.jsonl --out_folder ./ \" \\\n","          f\"--number_of_batches {MPNN_outputs_per_temperature} --temperature {temperature} \" \\\n","          f\"--omit_AA {MPNN_omit_AAs} --pdb_path {pdb_file} \" \\\n","          f\"--checkpoint_protein_mpnn {WDIR}/lib/LigandMPNN/model_params/ligandmpnn_v_32_010_25.pt\"\n","\n","    print(f\"Running LigandMPNN for {os.path.basename(pdb_file)} at temperature {temperature}\")\n","    print(cmd)\n","    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n","\n","    if result.returncode != 0:\n","        print(\"Error in LigandMPNN execution:\")\n","        print(result.stderr)\n","    else:\n","        print(\"LigandMPNN execution completed\")\n","\n","# Run LigandMPNN for each PDB file and temperature\n","for pdb_file in pdb_files:\n","    for temp in MPNN_temperatures:\n","        run_ligandmpnn(pdb_file, temp)\n","\n","# Create a .done file to indicate completion, regardless of success\n","with open(f\"{DESIGN_DIR_2nd_mpnn}/.done\", \"w\") as f:\n","    f.write(\"LigandMPNN runs completed\")\n","\n","print(\"LigandMPNN process finished. .done file created.\")\n","\n","# Verify the process\n","if os.path.exists(f\"{DESIGN_DIR_2nd_mpnn}/.done\"):\n","    print(\"2nd MPNN has been marked as completed.\")\n","else:\n","    print(\"2nd MPNN completion marker not found. This should not happen.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQpbIiGTxDSm"},"outputs":[],"source":["os.chdir(WDIR)\n","DESIGN_DIR_2nd_mpnn = f\"{WDIR}/4.1_2nd_mpnn\"\n","assert os.path.exists(DESIGN_DIR_2nd_mpnn+\"/.done\"), \"2nd MPNN has not been performed!\"\n","\n","AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n","os.makedirs(AF2_DIR, exist_ok=True)\n","os.chdir(AF2_DIR)\n","\n","### First collecting MPNN outputs and creating FASTA files for AF2 input\n","mpnn_fasta = utils.parse_fasta_files(glob.glob(f\"{DESIGN_DIR_2nd_mpnn}/seqs/*.fasta\"))\n","# Giving sequences unique names based on input PDB name, temperature, and sequence identifier\n","_mpnn_fasta = {}\n","for k, seq in mpnn_fasta.items():\n","    parts = k.split(\"_\")\n","    if \"global_score\" in k:\n","        _mpnn_fasta[f\"{parts[0]}_{parts[1]}_native\"] = seq.strip()\n","    else:\n","        temperature = parts[0].replace(\"T=\", \"T\")\n","        score = parts[1].split(\"=\")[1]\n","        sample = parts[3].replace(\"sample=\", \"\")\n","        _mpnn_fasta[f\"{temperature}_{score}_{sample}\"] = seq.strip()\n","\n","mpnn_fasta = {k:v for k,v in _mpnn_fasta.items()}\n","\n","print(f\"A total on {len(mpnn_fasta)} sequences will be predicted.\")\n","\n","## Splitting the MPNN sequences based on length\n","## and grouping them in smaller batches for each AF2 job\n","## Use group size of >40 when running on GPU. Also depends on how many sequences and resources you have.\n","SEQUENCES_PER_AF2_JOB = 5  # CPU\n","if USE_GPU_for_AF2 is True:\n","    SEQUENCES_PER_AF2_JOB = 100  # GPU\n","mpnn_fasta_split = utils.split_fasta_based_on_length(mpnn_fasta, SEQUENCES_PER_AF2_JOB, write_files=True)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iuIurA37xDSn"},"outputs":[],"source":["print(\"PYTHON dictionary contents:\")\n","print(PYTHON)\n","PYTHON['af2'] = \"python\"  # 또는 AlphaFold2용 Python 인터프리터의 전체 경로\n","PYTHON['af2'] = \"conda run -n alphafold python\"  # Conda 환경 'alphafold'를 사용하는 경우\n","print(\"AF2_script:\", AF2_script)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwHatp70xDSn"},"outputs":[],"source":["import os\n","import glob\n","\n","# AlphaFold2 설정\n","AF2_recycles = 3\n","AF2_models = \"4\"  # 필요한 경우 다른 모델을 이 문자열에 추가하세요\n","\n","# AlphaFold2 스크립트 경로 설정 (실제 경로로 변경해야 합니다)\n","AF2_script = \"/content/drive/MyDrive/특연/heme_binder_diffusion/scripts/af2/af2.py\"\n","\n","# Python 실행 경로 설정\n","PYTHON = {\"af2\": \"python\"}  # 또는 AlphaFold2를 실행할 Python 환경의 경로\n","\n","# AlphaFold2 명령어 생성\n","commands_af2 = []\n","for ff in glob.glob(\"*.fasta\"):\n","    cmd = f\"{PYTHON['af2']} {AF2_script} \" \\\n","          f\"--af-nrecycles {AF2_recycles} --af-models {AF2_models} \" \\\n","          f\"--fasta {ff} --scorefile {ff.replace('.fasta', '.csv')}\"\n","    commands_af2.append(cmd)\n","\n","print(f\"총 {len(commands_af2)}개의 AlphaFold2 작업이 준비되었습니다.\")\n","print(\"예시 AlphaFold2 명령어:\")\n","print(commands_af2[0] if commands_af2 else \"명령어가 생성되지 않았습니다.\")\n","\n","# AlphaFold2 실행 (주의: 시간이 오래 걸릴 수 있습니다)\n","for i, cmd in enumerate(commands_af2, 1):\n","    print(f\"작업 {i}/{len(commands_af2)} 실행 중...\")\n","    os.system(cmd)\n","\n","print(\"모든 AlphaFold2 작업이 완료되었습니다.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RqZh0-2ExDSo"},"outputs":[],"source":["## If you're done with AF2 and happy with the outputs then mark it as done\n","AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n","os.chdir(AF2_DIR)\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","\n","if not os.path.exists(AF2_DIR+\"/.done\"):\n","    with open(f\"{AF2_DIR}/.done\", \"w\") as file:\n","        file.write(f\"Run user: {username}\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DgHtwNgExDSo"},"outputs":[],"source":["PYTHON = {\n","    \"diffusion\": \"/usr/bin/python3\",\n","    \"af2\": \"/usr/bin/python3\",\n","    \"proteinMPNN\": \"/usr/bin/python3\",\n","    \"general\": \"/usr/bin/python3\"\n","}\n","\n","# Combining all CSV scorefiles into one\n","os.system(\"head -n 1 $(ls *aa*.csv | shuf -n 1) > scores.csv ; for f in *aa*.csv ; do tail -n +2 ${f} >> scores.csv ; done\")\n","assert os.path.exists(\"scores.csv\"), \"Could not combine scorefiles\"\n","\n","### Calculating the RMSDs of AF2 predictions relative to the diffusion outputs\n","### Catalytic residue sidechain RMSDs are calculated in the reference PDB has REMARK 666 line present\n","\n","analysis_cmd = f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/utils/analyze_af2.py --scorefile scores.csv \"\\\n","               f\"--ref_path {DESIGN_DIR_ligMPNN}/good/ --mpnn --params {' '.join(params)}\"\n","\n","p = subprocess.Popen(analysis_cmd, shell=True)\n","(output, err) = p.communicate()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_igXDUDxDSo"},"outputs":[],"source":["scores_af2 = pd.read_csv(\"scores.sc\", sep=\",\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D4W2K_04xDSo"},"outputs":[],"source":["print(\"Columns in scores_af2:\")\n","print(scores_af2.columns)\n","print(\"\\nFirst few rows of scores_af2:\")\n","print(scores_af2.head())\n","\n","# 필요한 경우 열 이름 공백 제거\n","scores_af2.columns = scores_af2.columns.str.strip()\n","\n","# 데이터 타입 변환 (필요한 경우)\n","scores_af2['lDDT'] = pd.to_numeric(scores_af2['lDDT'], errors='coerce')\n","scores_af2['rmsd'] = pd.to_numeric(scores_af2['rmsd'], errors='coerce')\n","scores_af2['rmsd_SR1'] = pd.to_numeric(scores_af2['rmsd_SR1'], errors='coerce')\n","\n","print(\"\\nUpdated columns in scores_af2:\")\n","print(scores_af2.columns)\n","print(\"\\nUpdated first few rows of scores_af2:\")\n","print(scores_af2.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJl-hEZExDSp"},"outputs":[],"source":["# NaN 값 제거\n","scores_af2 = scores_af2.dropna(subset=['lDDT', 'rmsd'])\n","\n","# 데이터 타입 변환\n","scores_af2['lDDT'] = pd.to_numeric(scores_af2['lDDT'], errors='coerce')\n","scores_af2['rmsd'] = pd.to_numeric(scores_af2['rmsd'], errors='coerce')\n","\n","# 이상치 제거 (선택사항)\n","q_low = scores_af2['lDDT'].quantile(0.01)\n","q_high = scores_af2['lDDT'].quantile(0.99)\n","scores_af2 = scores_af2[(scores_af2['lDDT'] > q_low) & (scores_af2['lDDT'] < q_high)]\n","\n","q_low = scores_af2['rmsd'].quantile(0.01)\n","q_high = scores_af2['rmsd'].quantile(0.99)\n","scores_af2 = scores_af2[(scores_af2['rmsd'] > q_low) & (scores_af2['rmsd'] < q_high)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLRWw9Gq5JXv"},"outputs":[],"source":["# 열 이름 확인 후 필터 정의\n","AF2_filters = {\"lDDT\": [3.8, \">=\"],  # 값이 매우 작아 보이므로 조정\n","               \"rmsd\": [20.0, \"<=\"],  # 값이 크므로 조정\n","               \"rmsd_SR1\": [0.1, \"<=\"]}  # 값이 0이므로 조정\n","\n","scores_af2_filtered = utils.filter_scores(scores_af2, AF2_filters)\n","utils.dump_scorefile(scores_af2_filtered, \"filtered_scores.sc\")\n","\n","## Plotting AF2 scores\n","plt.figure(figsize=(12, 3))\n","for i, k in enumerate(AF2_filters):\n","    plt.subplot(1, 3, i+1)\n","    plt.hist(scores_af2[k])\n","    plt.title(k)\n","    plt.xlabel(k)\n","plt.tight_layout()\n","plt.show()\n","\n","utils.plot_score_pairs(scores_af2, \"lDDT\", \"rmsd\", AF2_filters[\"lDDT\"][0], AF2_filters[\"rmsd\"][0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KPaepol_AE4s"},"outputs":[],"source":["import os\n","import glob\n","\n","# 'good' 폴더 경로 설정\n","good_folder = os.path.join(AF2_DIR, '2_af2', 'good')\n","\n","# 'good' 폴더에서 모든 PDB 파일 찾기\n","good_af2_models = glob.glob(os.path.join(good_folder, '*.pdb'))\n","\n","print(f\"Found {len(good_af2_models)} PDB files in the 'good' folder.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"268bceef"},"outputs":[],"source":["AF2_DIR = f\"{WDIR}/5.1_2nd_af2\"\n","DESIGN_DIR_ligMPNN = f\"{WDIR}/3.1_design_pocket_ligandMPNN\"\n","assert len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")) > 0, \"No good AF2 models to relax with\"\n","\n","os.chdir(WDIR)\n","RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n","os.makedirs(RELAX_DIR, exist_ok=True)\n","os.chdir(RELAX_DIR)\n","\n","os.makedirs(RELAX_DIR+\"/logs\", exist_ok=True)\n","\n","## First matching up the AF2 output filenames of step 5 with pocket design filenames from step 3\n","ref_and_model_pairs = []\n","for r in glob.glob(f\"{DESIGN_DIR_ligMPNN}/good/*.pdb\"):\n","    for pdbfile in glob.glob(f\"{AF2_DIR}/good/*.pdb\"):\n","        if os.path.basename(r).replace(\".pdb\", \"_\") in pdbfile:\n","            ref_and_model_pairs.append((r, pdbfile))\n","\n","assert len(ref_and_model_pairs) == len(glob.glob(f\"{AF2_DIR}/good/*.pdb\")), \"Was not able to match all models with reference structures\"\n","\n","\n","## Generating commands for relax jobs\n","### Performing 1 relax iteration on each input structure\n","NSTRUCT = 1\n","cstfile = f\"{SCRIPT_DIR}/theozyme/HBA/HBA_CYS_UPO.cst\"\n","\n","commands_relax = []\n","cmds_filename_rlx = \"commands_relax\"\n","with open(cmds_filename_rlx, \"w\") as file:\n","    for r_m in ref_and_model_pairs:\n","        commands_relax.append(f\"{PYTHON['general']} {SCRIPT_DIR}/scripts/design/align_add_ligand_relax.py \"\n","                              f\"--outdir ./ --ligand {LIGAND} --ref_pdb {r_m[0]} \"\n","                              f\"--pdb {r_m[1]} --nstruct {NSTRUCT} \"\n","                              f\"--params {' '.join(params)} --cstfile {cstfile} > logs/{os.path.basename(r_m[1]).replace('.pdb', '.log')}\\n\")\n","        file.write(commands_relax[-1])\n","\n","print(\"Example design command:\")\n","print(commands_relax[-1])\n","\n","\n","### Running design jobs with Slurm.\n","submit_script = \"submit_relax.sh\"\n","utils.create_slurm_submit_script(filename=submit_script, name=\"6.1_final_relax\", mem=\"4g\",\n","                                 N_cores=1, time=\"0:30:00\", email=EMAIL, array=len(commands_relax),\n","                                 array_commandfile=cmds_filename_rlx)\n","\n","if not os.path.exists(RELAX_DIR+\"/.done\"):\n","    p = subprocess.Popen(['sbatch', submit_script], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","    (output, err) = p.communicate()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"366ce521"},"outputs":[],"source":["## If you're done with final relax and happy with the outputs then mark it as done\n","RELAX_DIR = f\"{WDIR}/6.1_final_relax\"\n","os.chdir(RELAX_DIR)\n","\n","if not os.path.exists(RELAX_DIR+\"/.done\"):\n","    with open(f\"{RELAX_DIR}/.done\", \"w\") as file:\n","        file.write(f\"Run user: {username}\\n\")"]}]}