{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["#이 부분부터는 안면 인식 이후에 성별과 연령을 감지하는 코드입니다\n","#데이터를 학습시키지 않으면 성공률이 낮아서 다른 github 코드를 참고했어요\n","!git clone https://github.com/habibrayhan007/Age-and-gender-detection.git"],"metadata":{"id":"1_1ZJf0R4iOX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ry9ZK5Ig3oPv"},"outputs":[],"source":["#openCV1 - 얼굴을 감지하는 코드입니다...사람 얼굴이 몇 개 있는지 openCV(computer vision)의 haar cascade mechanism을 이용합니다\n","import cv2 #openCV - computer vision library\n","import matplotlib.pyplot as plt\n","from google.colab import files\n","import io\n","from PIL import Image\n","import numpy as np\n","\n","# 이미지 업로드 위젯\n","uploaded = files.upload()\n","\n","# 업로드한 이미지 로드 및 변환\n","for fn in uploaded.keys():\n","    image_path = fn\n","\n","# PIL 이미지를 OpenCV 형식으로 변환\n","image = Image.open(io.BytesIO(uploaded[image_path]))\n","image_cv = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2BGR)\n","\n","# 얼굴 인식용 OpenCV Haar Cascade 로드\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","# 그레이스케일 변환\n","gray = cv2.cvtColor(image_cv, cv2.COLOR_BGR2GRAY)\n","\n","# 얼굴 검출\n","faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n","\n","# 검출된 얼굴에 사각형 그리기\n","for (x, y, w, h) in faces:\n","    cv2.rectangle(image_cv, (x, y), (x+w, y+h), (255, 0, 0), 2)\n","\n","# 결과 이미지 표시\n","plt.imshow(cv2.cvtColor(image_cv, cv2.COLOR_BGR2RGB))\n","plt.axis('off')\n","plt.show()\n","\n","# 검출된 얼굴 수 출력\n","print(f\"Detected {len(faces)} face(s).\")\n"]},{"cell_type":"code","source":["%cd Age-and-gender-detection"],"metadata":{"id":"JDFcGyTn4vk3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown https://drive.google.com/uc?id=1_aDScOvBeBLCn_iv0oxSO8X1ySQpSbIS"],"metadata":{"id":"v9L9g7KA4yq7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip modelNweight.zip"],"metadata":{"id":"-9oAHist43mt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import libraries\n","\n","import cv2 as cv\n","from google.colab.patches import cv2_imshow\n","import math\n","import time"],"metadata":{"id":"kn1MtM1u45yw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#코드가 돌아가다가 멈추는 것처럼 느껴지실 수 있는데 사진 업로드까지 하셔야 다 돌아갑니다!\n","#사진의 경우에 너무 어둡지 않는 정면 사진을 더 잘 인식하는 것 같아요:)\n","import cv2 as cv\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","from google.colab import files\n","\n","def getFaceBox(net, frame, conf_threshold=0.7):\n","    frameOpencvDnn = frame.copy()\n","    frameHeight = frameOpencvDnn.shape[0]\n","    frameWidth = frameOpencvDnn.shape[1]\n","    blob = cv.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","    net.setInput(blob)\n","    detections = net.forward()\n","    bboxes = []\n","    for i in range(detections.shape[2]):\n","        confidence = detections[0, 0, i, 2]\n","        if confidence > conf_threshold:\n","            x1 = int(detections[0, 0, i, 3] * frameWidth)\n","            y1 = int(detections[0, 0, i, 4] * frameHeight)\n","            x2 = int(detections[0, 0, i, 5] * frameWidth)\n","            y2 = int(detections[0, 0, i, 6] * frameHeight)\n","            bboxes.append([x1, y1, x2, y2])\n","            cv.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n","    return frameOpencvDnn, bboxes\n","\n","faceProto = \"modelNweight/opencv_face_detector.pbtxt\"\n","faceModel = \"modelNweight/opencv_face_detector_uint8.pb\"\n","\n","ageProto = \"modelNweight/age_deploy.prototxt\"\n","ageModel = \"modelNweight/age_net.caffemodel\"\n","\n","genderProto = \"modelNweight/gender_deploy.prototxt\"\n","genderModel = \"modelNweight/gender_net.caffemodel\"\n","\n","MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n","ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n","genderList = ['Male', 'Female']\n","\n","# Load network\n","ageNet = cv.dnn.readNet(ageModel, ageProto)\n","genderNet = cv.dnn.readNet(genderModel, genderProto)\n","faceNet = cv.dnn.readNet(faceModel, faceProto)\n","\n","padding = 20\n","\n","def age_gender_detector(frame):\n","    frameFace, bboxes = getFaceBox(faceNet, frame)\n","    for bbox in bboxes:\n","        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n","\n","        blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n","        genderNet.setInput(blob)\n","        genderPreds = genderNet.forward()\n","        gender = genderList[genderPreds[0].argmax()]\n","        ageNet.setInput(blob)\n","        agePreds = ageNet.forward()\n","        age = ageList[agePreds[0].argmax()]\n","\n","        label = \"{},{}\".format(gender, age)\n","        cv.putText(frameFace, label, (bbox[0], bbox[1]-10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv.LINE_AA)\n","    return frameFace\n","\n","# 위젯을 통한 이미지 업로드 및 처리\n","def handle_uploaded_image(file):\n","    img = cv.imread(file)\n","    result_img = age_gender_detector(img)\n","    cv2_imshow(result_img)\n","\n","# 위젯 생성 및 이미지 업로드 처리\n","uploaded = files.upload()\n","for filename in uploaded.keys():\n","    print('Uploaded file:', filename)\n","    handle_uploaded_image(filename)\n"],"metadata":{"id":"QagwlYqT47_M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#모델을 학습시킬 때 사용한 사진의 예시입니다.\n","input = cv.imread(\"habib.jpg\")\n","output = age_gender_detector(input)\n","cv2_imshow(output)"],"metadata":{"id":"I_WJtvqy5Js-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!gdown https://drive.google.com/uc?id=1_aDScOvBeBLCn_iv0oxSO8X1ySQpSbIS\n","#학습하는 데이터가 대강 90메가"],"metadata":{"id":"TdNf5kN6BfB5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip modelNweight.zip"],"metadata":{"id":"EYaYDPWYBfJ9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#import libraries\n","\n","import cv2 as cv\n","from google.colab.patches import cv2_imshow\n","import math\n","import time"],"metadata":{"id":"p45F761zBfSY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install opencv-python opencv-python-headless moviepy numpy"],"metadata":{"id":"_CjaD4RxBfa_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def apply_cat_filter(frame, bbox, filter_img):\n","    filter_height, filter_width = filter_img.shape[:2]\n","    face_width = bbox[2] - bbox[0]\n","    face_height = bbox[3] - bbox[1]\n","\n","    # Resize the filter to fit the face\n","    factor = face_width / filter_width\n","    filter_resized = cv.resize(filter_img, (0, 0), fx=factor, fy=factor)\n","\n","    filter_height, filter_width = filter_resized.shape[:2]\n","\n","    # Calculate the position to place the filter\n","    x1 = bbox[0] - (filter_width - face_width) // 2\n","    y1 = bbox[1] - filter_height // 2\n","    x2 = x1 + filter_width\n","    y2 = y1 + filter_height\n","\n","    # Check if the filter is out of bounds and adjust\n","    x1 = max(x1, 0)\n","    y1 = max(y1, 0)\n","    x2 = min(x2, frame.shape[1])\n","    y2 = min(y2, frame.shape[0])\n","\n","    filter_resized = filter_resized[0:(y2-y1), 0:(x2-x1)]\n","\n","    # Split the filter image into its color and alpha components\n","    filter_rgb = filter_resized[..., :3]\n","    filter_alpha = filter_resized[..., 3] / 255.0\n","\n","    # Get the region of interest from the frame\n","    roi = frame[y1:y2, x1:x2]\n","\n","    # Blend the filter with the ROI using the alpha mask\n","    for c in range(0, 3):\n","        roi[:, :, c] = roi[:, :, c] * (1 - filter_alpha) + filter_rgb[:, :, c] * filter_alpha\n","\n","    frame[y1:y2, x1:x2] = roi\n","\n","    return frame\n"],"metadata":{"id":"pEF7J7qouZb-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files, output\n","import cv2 as cv\n","import numpy as np\n","from moviepy.editor import *\n","import threading\n","import time\n","import base64\n","from IPython.display import display, Javascript, Image\n","\n","# Load the pre-trained models\n","faceProto = \"modelNweight/opencv_face_detector.pbtxt\"\n","faceModel = \"modelNweight/opencv_face_detector_uint8.pb\"\n","ageProto = \"modelNweight/age_deploy.prototxt\"\n","ageModel = \"modelNweight/age_net.caffemodel\"\n","genderProto = \"modelNweight/gender_deploy.prototxt\"\n","genderModel = \"modelNweight/gender_net.caffemodel\"\n","\n","MODEL_MEAN_VALUES = (78.4263377603, 87.7689143744, 114.895847746)\n","ageList = ['(0-2)', '(4-6)', '(8-12)', '(15-20)', '(25-32)', '(38-43)', '(48-53)', '(60-100)']\n","genderList = ['Male', 'Female']\n","\n","# Load networks\n","ageNet = cv.dnn.readNet(ageModel, ageProto)\n","genderNet = cv.dnn.readNet(genderModel, genderProto)\n","faceNet = cv.dnn.readNet(faceModel, faceProto)\n","\n","padding = 20\n","\n","def getFaceBox(net, frame, conf_threshold=0.7):\n","    frameOpencvDnn = frame.copy()\n","    frameHeight = frameOpencvDnn.shape[0]\n","    frameWidth = frameOpencvDnn.shape[1]\n","    blob = cv.dnn.blobFromImage(frameOpencvDnn, 1.0, (300, 300), [104, 117, 123], True, False)\n","\n","    net.setInput(blob)\n","    detections = net.forward()\n","    bboxes = []\n","    for i in range(detections.shape[2]):\n","        confidence = detections[0, 0, i, 2]\n","        if confidence > conf_threshold:\n","            x1 = int(detections[0, 0, i, 3] * frameWidth)\n","            y1 = int(detections[0, 0, i, 4] * frameHeight)\n","            x2 = int(detections[0, 0, i, 5] * frameWidth)\n","            y2 = int(detections[0, 0, i, 6] * frameHeight)\n","            bboxes.append([x1, y1, x2, y2])\n","            cv.rectangle(frameOpencvDnn, (x1, y1), (x2, y2), (0, 255, 0), int(round(frameHeight/150)), 8)\n","    return frameOpencvDnn, bboxes\n","\n","def apply_cat_filter(frame, bbox, filter_img):\n","    filter_height, filter_width = filter_img.shape[:2]\n","    face_width = bbox[2] - bbox[0]\n","    face_height = bbox[3] - bbox[1]\n","\n","    # Resize the filter to fit the face\n","    factor = face_width / filter_width\n","    filter_resized = cv.resize(filter_img, (0, 0), fx=factor, fy=factor)\n","\n","    filter_height, filter_width = filter_resized.shape[:2]\n","\n","    # Calculate the position to place the filter\n","    x1 = bbox[0] - (filter_width - face_width) // 2\n","    y1 = bbox[1] - filter_height // 2\n","    x2 = x1 + filter_width\n","    y2 = y1 + filter_height\n","\n","    # Check if the filter is out of bounds and adjust\n","    x1 = max(x1, 0)\n","    y1 = max(y1, 0)\n","    x2 = min(x2, frame.shape[1])\n","    y2 = min(y2, frame.shape[0])\n","\n","    filter_resized = filter_resized[0:(y2-y1), 0:(x2-x1)]\n","\n","    # Split the filter image into its color and alpha components\n","    filter_rgb = filter_resized[..., :3]\n","    filter_alpha = filter_resized[..., 3] / 255.0\n","\n","    # Get the region of interest from the frame\n","    roi = frame[y1:y2, x1:x2]\n","\n","    # Blend the filter with the ROI using the alpha mask\n","    for c in range(0, 3):\n","        roi[:, :, c] = roi[:, :, c] * (1 - filter_alpha) + filter_rgb[:, :, c] * filter_alpha\n","\n","    frame[y1:y2, x1:x2] = roi\n","\n","    return frame\n","\n","def age_gender_detector(frame, filter_img):\n","    frameFace, bboxes = getFaceBox(faceNet, frame)\n","    for bbox in bboxes:\n","        face = frame[max(0,bbox[1]-padding):min(bbox[3]+padding,frame.shape[0]-1),max(0,bbox[0]-padding):min(bbox[2]+padding, frame.shape[1]-1)]\n","\n","        blob = cv.dnn.blobFromImage(face, 1.0, (227, 227), MODEL_MEAN_VALUES, swapRB=False)\n","        genderNet.setInput(blob)\n","        genderPreds = genderNet.forward()\n","        gender = genderList[genderPreds[0].argmax()]\n","        ageNet.setInput(blob)\n","        agePreds = ageNet.forward()\n","        age = ageList[agePreds[0].argmax()]\n","\n","        label = \"{},{}\".format(gender, age)\n","        cv.putText(frameFace, label, (bbox[0], bbox[1]-10), cv.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2, cv.LINE_AA)\n","\n","        # Apply cat filter\n","        frameFace = apply_cat_filter(frameFace, bbox, filter_img)\n","\n","    return frameFace\n","\n","def play_bgm(bgm_path):\n","    audio_clip = AudioFileClip(bgm_path)\n","    audio_clip.preview()\n","\n","def capture_video():\n","    display(Javascript('''\n","        async function captureVideo() {\n","            const video = document.createElement('video');\n","            const stream = await navigator.mediaDevices.getUserMedia({ video: true });\n","            video.srcObject = stream;\n","            document.body.appendChild(video);\n","            video.play();\n","            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n","\n","            // Capture a frame every 100ms and send to Python\n","            const captureFrame = async () => {\n","                if (!video.srcObject) return;\n","                const canvas = document.createElement('canvas');\n","                canvas.width = video.videoWidth;\n","                canvas.height = video.videoHeight;\n","                const context = canvas.getContext('2d');\n","                context.drawImage(video, 0, 0, canvas.width, canvas.height);\n","                const dataUrl = canvas.toDataURL('image/jpeg');\n","                google.colab.kernel.invokeFunction('notebook.processFrame', [dataUrl], {});\n","                setTimeout(captureFrame, 100);\n","            };\n","            captureFrame();\n","\n","            const stopButton = document.createElement('button');\n","            stopButton.textContent = 'Stop Capture';\n","            document.body.appendChild(stopButton);\n","\n","            stopButton.onclick = () => {\n","                stream.getTracks().forEach(track => track.stop());\n","                video.remove();\n","                stopButton.remove();\n","            };\n","        }\n","        captureVideo();\n","    '''))\n","\n","def process_frame(dataUrl):\n","    encoded_data = dataUrl.split(',')[1]\n","    img_data = base64.b64decode(encoded_data)\n","    np_arr = np.frombuffer(img_data, np.uint8)\n","    frame = cv.imdecode(np_arr, cv.IMREAD_COLOR)\n","\n","    result_frame = age_gender_detector(frame, filter_img)\n","\n","    _, jpeg = cv.imencode('.jpg', result_frame)\n","    display_ip = display(Image(data=jpeg.tobytes()), display_id='video_frame', update=True)\n","\n","    # Return the result frame for further processing if needed\n","    return result_frame\n","\n","output.register_callback('notebook.processFrame', process_frame)\n","\n","# 필터 및 BGM 업로드\n","print(\"필터 이미지를 업로드해주세요 (PNG 형식):\")\n","uploaded_filter = files.upload()\n","filter_path = list(uploaded_filter.keys())[0]\n","\n","print(\"BGM 파일을 업로드해주세요 (MP3 형식):\")\n","uploaded_bgm = files.upload()\n","bgm_path = list(uploaded_bgm.keys())[0]\n","\n","# 필터 이미지 로드\n","filter_img = cv.imread(filter_path, cv.IMREAD_UNCHANGED)\n","if filter_img is None:\n","    raise ValueError(\"필터 이미지를 로드할 수 없습니다.\")\n","\n","# BGM 재생 스레드 시작\n","bgm_thread = threading.Thread(target=play_bgm, args=(bgm_path,))\n","bgm_thread.start()\n","\n","# 웹캠 영상 캡처 시작\n","capture_video()\n"],"metadata":{"id":"OBP6luSGudV1"},"execution_count":null,"outputs":[]}]}